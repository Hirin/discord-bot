"""
LLM Service - GLM API Client
Uses OpenAI-compatible client with Z.AI base URL
"""

import asyncio
import logging
import os
from typing import Optional

from openai import OpenAI

logger = logging.getLogger(__name__)


def get_client() -> OpenAI:
    """Get configured OpenAI client for GLM API"""
    return OpenAI(
        api_key=os.getenv("GLM_API_KEY"),
        base_url=os.getenv("GLM_BASE_URL", "https://api.z.ai/api/paas/v4/"),
    )


async def summarize_transcript(
    transcript: str, timeout: int = 60, retries: int = 3
) -> Optional[str]:
    """
    Summarize a meeting transcript using GLM API.

    Args:
        transcript: Full transcript text
        timeout: Timeout in seconds
        retries: Number of retry attempts

    Returns:
        Summary text or None if failed
    """
    model = os.getenv("GLM_MODEL", "glm-4.6")
    client = get_client()

    system_prompt = """Báº¡n lÃ  trá»£ lÃ½ tÃ³m táº¯t cuá»™c há»p chuyÃªn nghiá»‡p. 
HÃ£y tÃ³m táº¯t cuá»™c há»p theo cáº¥u trÃºc:

## ğŸ“‹ TÃ³m táº¯t tá»•ng quan
(2-3 cÃ¢u vá» ná»™i dung chÃ­nh)

## ğŸ¯ CÃ¡c Ä‘iá»ƒm chÃ­nh
- Äiá»ƒm 1
- Äiá»ƒm 2
...

## âœ… Quyáº¿t Ä‘á»‹nh & Action Items
- [NgÆ°á»i] - Viá»‡c cáº§n lÃ m

## ğŸ“Œ Ghi chÃº quan trá»ng
(Náº¿u cÃ³)

HÃ£y tÃ³m táº¯t ngáº¯n gá»n, sÃºc tÃ­ch, báº±ng tiáº¿ng Viá»‡t."""

    for attempt in range(retries):
        try:
            logger.info(f"Summarizing transcript (attempt {attempt + 1})...")

            # Run sync client in thread pool
            loop = asyncio.get_event_loop()
            completion = await loop.run_in_executor(
                None,
                lambda: client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {
                            "role": "user",
                            "content": f"TÃ³m táº¯t cuá»™c há»p sau:\n\n{transcript[:15000]}",
                        },  # Limit context
                    ],
                    timeout=timeout,
                ),
            )

            summary = completion.choices[0].message.content
            logger.info(f"Summary generated: {len(summary)} chars")
            return summary

        except Exception as e:
            logger.error(f"LLM attempt {attempt + 1} failed: {e}")
            if attempt < retries - 1:
                backoff = 2**attempt
                logger.info(f"Retrying in {backoff}s...")
                await asyncio.sleep(backoff)

    return None


def get_fallback_template() -> str:
    """Return fallback template when LLM fails"""
    return """âš ï¸ **KhÃ´ng thá»ƒ táº¡o tÃ³m táº¯t tá»± Ä‘á»™ng**

Vui lÃ²ng Ä‘iá»n thá»§ cÃ´ng:

## ğŸ“‹ TÃ³m táº¯t tá»•ng quan
- Cuá»™c há»p vá»: ___
- Thá»i gian: ___

## ğŸ¯ CÃ¡c Ä‘iá»ƒm chÃ­nh
- [ ] ___
- [ ] ___

## âœ… Action Items
- [ ] NgÆ°á»i: ___ | Viá»‡c: ___

## ğŸ“Œ Ghi chÃº
- ___
"""
